{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavankalyano76/PavanKalyan_INFO5731_Spring2025/blob/main/Natukula_Pavan_Kalyan_Assignment02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Monday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDyTKYs-yGit"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "from urllib.error import HTTPError\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "pages = 41\n",
        "dict = {}\n",
        "narr_count = 0\n",
        "#main_url = \"https://ddr.densho.org/narrators/?page={}\"\n",
        "for i in range(1,pages+1):\n",
        "\n",
        "    main_url = f'https://ddr.densho.org/narrators/?page={i}'\n",
        "    link1 = Request(main_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    url1 = urlopen(link1)\n",
        "\n",
        "    data1 = url1.read()\n",
        "    #data1_soup = BeautifulSoup(data1)\n",
        "\n",
        "    soup = BeautifulSoup(data1,\"html.parser\")\n",
        "    #print(type(soup))\n",
        "\n",
        "    div_clause = soup.find(id = \"list_tab\")\n",
        "\n",
        "\n",
        "    narrators = div_clause.find_all(class_='media-body')\n",
        "    #print(narrators)\n",
        "\n",
        "    for i in narrators:\n",
        "        if len(dict) == 904:\n",
        "            break\n",
        "        tags=i.find(\"a\")\n",
        "        tag2 = i.find(class_ = \"source muted\")\n",
        "        name = tags.get_text()\n",
        "        details = tag2.get_text().strip()\n",
        "        dict[name] = details\n",
        "        narr_count+=1\n",
        "\n",
        "    if len(dict) == 904:\n",
        "        break\n",
        "#print(dict)\n",
        "df = pd.DataFrame(list(dict.items()), columns=[\"Name\", \"Details\"])\n",
        "#df = pd.DataFrame(dict, columns=[\"Name\", \"Details\"])\n",
        "df\n",
        "\n",
        "df.to_csv(\"Narrator Information.csv\",index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QX6bJjGWXY9",
        "outputId": "c3858d03-aecb-465c-a393-f0a8fc96988f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Write code for each of the sub parts with proper comments.\n",
        "import pandas as pd\n",
        "df = pd.read_csv('Narrator Information.csv')\n",
        "df = df.dropna()\n",
        "\n",
        "#1. Removing Noise\n",
        "df['CleanedName'] = df['Name'].str.replace('[^\\w\\s]','',regex=True)\n",
        "df['CleanedDetails'] = df['Details'].str.replace('[^\\w\\s]','',regex=True)\n",
        "\n",
        "#2. Removing Numbers\n",
        "df['CleanedName'] = df['CleanedName'].str.replace('\\d+', '', regex=True)\n",
        "df['CleanedDetails'] = df['CleanedDetails'].str.replace('\\d+', '', regex=True)\n",
        "\n",
        "#3. Removing Stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "df['CleanedName'] = df['CleanedName'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "df['CleanedDetails'] = df['CleanedDetails'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "\n",
        "#4. Lowercase all texts\n",
        "df['CleanedName'] = df['CleanedName'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "df['CleanedDetails'] = df['CleanedDetails'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "\n",
        "#5. Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "st = PorterStemmer()\n",
        "df['CleanedName']=df['CleanedName'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "df['CleanedDetails']=df['CleanedDetails'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "\n",
        "\n",
        "#6. Lemmatization\n",
        "from textblob import Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df['CleanedName'] = df['CleanedName'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "df['CleanedDetails'] = df['CleanedDetails'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "df.head()\n",
        "df.to_csv('Narrators_Information_Cleaned.csv', index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0oOSlsOS0cq",
        "outputId": "d4a48570-c592-45b7-fe15-670dce2feeca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Nouns: 8896\n",
            "Total Verbs: 2060\n",
            "Total Adjectives: 2733\n",
            "Total Adverbs: 466\n"
          ]
        }
      ],
      "source": [
        "########################### PART I ##################################\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab data for sentence tokenization\n",
        "\n",
        "# Load CSV file\n",
        "\n",
        "df_pos= pd.read_csv('/content/Narrators_Information_Cleaned.csv')\n",
        "\n",
        "# UDF for tagging and countings pos\n",
        "def pos_tagging(text):\n",
        "    tokens = word_tokenize(str(text))  # Tokenize the text\n",
        "    tagged_words = pos_tag(tokens)  # POS tagging\n",
        "\n",
        "    # Counter is a class in collection that creates a dictionary with pos tag and count in partucular text\n",
        "    pos_counts_dict = Counter(tag for word, tag in tagged_words)\n",
        "\n",
        "    # Map NLTK POS tags to simplified categories\n",
        "    countOfNouns = sum(pos_counts_dict[tag] for tag in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"])  # Nouns\n",
        "    countOfVerbs = sum(pos_counts_dict[tag] for tag in [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"])  # Verbs\n",
        "    countOfAdjectives = sum(pos_counts_dict[tag] for tag in [\"JJ\", \"JJR\", \"JJS\"])  # Adjectives\n",
        "    countOfAdverbs = sum(pos_counts_dict[tag] for tag in [\"RB\", \"RBR\", \"RBS\"])  # Adverbs\n",
        "\n",
        "    return countOfNouns, countOfVerbs, countOfAdjectives, countOfAdverbs\n",
        "\n",
        "# Apply pos tagging function to \"CleanedDetails\" column\n",
        "df_pos[['Nouns', 'Verbs', 'Adjectives', 'Adverbs']] = pd.DataFrame(df_pos[\"CleanedDetails\"].apply(pos_tagging).to_list(), index=df_pos.index)\n",
        "\n",
        "\n",
        "# Sum total occurrences across all rows\n",
        "total_nouns = df_pos[\"Nouns\"].sum()\n",
        "total_verbs = df_pos[\"Verbs\"].sum()\n",
        "total_adjectives = df_pos[\"Adjectives\"].sum()\n",
        "total_adverbs = df_pos[\"Adverbs\"].sum()\n",
        "\n",
        "# Output the results\n",
        "print(f\"Total Nouns: {total_nouns}\")\n",
        "print(f\"Total Verbs: {total_verbs}\")\n",
        "print(f\"Total Adjectives: {total_adjectives}\")\n",
        "print(f\"Total Adverbs: {total_adverbs}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import benepar\n",
        "from spacy import displacy\n",
        "from nltk import Tree\n",
        "\n",
        "\n",
        "# Load spaCy model and benepar\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "benepar.download('benepar_en3')\n",
        "nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "\n",
        "\n",
        "def plot_dependency_tree(doc):\n",
        "    print(\"\\nDependency Parsing Tree (Text Representation):\")\n",
        "    for token in doc:\n",
        "        print(f\"{token.text} --({token.dep_} → {spacy.explain(token.dep_)})--> {token.head.text}\")\n",
        "\n",
        "    # Visual dependency tree using displaCy\n",
        "    displacy.render(doc, style='dep', jupyter=True, options={'compact': True, 'distance': 90})\n",
        "\n",
        "\n",
        "def plot_constituency_tree(doc):\n",
        "    for sent in doc.sents:\n",
        "        print(\"\\nConstituency Parsing Tree (Text Representation):\")\n",
        "        print(sent._.parse_string)\n",
        "\n",
        "        # Visual constituency tree with nltk\n",
        "        tree = Tree.fromstring(sent._.parse_string)\n",
        "        tree.pretty_print()\n",
        "\n",
        "\n",
        "\n",
        "sentence = df_pos['CleanedDetails'][0]\n",
        "doc = nlp(sentence)\n",
        "\n",
        "print(f\"Sentence: {sentence}\")\n",
        "\n",
        "plot_dependency_tree(doc)\n",
        "plot_constituency_tree(doc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9pRMdiZVy6-D",
        "outputId": "1ddac614-e845-4541-e3f7-aa389cb61c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/benepar/parse_chart.py:169: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\n",
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: nisei femal born may selleck washington spent much childhood beaverton oregon father own farm influenc earli\n",
            "\n",
            "Dependency Parsing Tree (Text Representation):\n",
            "nisei --(compound → compound)--> femal\n",
            "femal --(nsubj → nominal subject)--> selleck\n",
            "born --(acl → clausal modifier of noun (adjectival clause))--> femal\n",
            "may --(aux → auxiliary)--> selleck\n",
            "selleck --(compound → compound)--> washington\n",
            "washington --(nsubj → nominal subject)--> spent\n",
            "spent --(ROOT → root)--> spent\n",
            "much --(amod → adjectival modifier)--> beaverton\n",
            "childhood --(compound → compound)--> beaverton\n",
            "beaverton --(dobj → direct object)--> spent\n",
            "oregon --(compound → compound)--> father\n",
            "father --(appos → appositional modifier)--> beaverton\n",
            "own --(amod → adjectival modifier)--> earli\n",
            "farm --(compound → compound)--> influenc\n",
            "influenc --(compound → compound)--> earli\n",
            "earli --(npadvmod → noun phrase as adverbial modifier)--> spent\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/distributions/distribution.py:56: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"72c9ea1ee15747e0ac9714f1803f2384-0\" class=\"displacy\" width=\"1490\" height=\"317.0\" direction=\"ltr\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">nisei</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">femal</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">born</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">may</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">selleck</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">washington</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">spent</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">much</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">childhood</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">beaverton</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">oregon</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1040\">father</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1040\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">own</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1220\">farm</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1220\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1310\">influenc</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1310\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1400\">earli</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1400\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-72c9ea1ee15747e0ac9714f1803f2384-0-0\" stroke-width=\"2px\" d=\"M62,182.0 62,167.0 131.0,167.0 131.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-72c9ea1ee15747e0ac9714f1803f2384-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M62,184.0 L58,176.0 66,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-72c9ea1ee15747e0ac9714f1803f2384-0-1\" stroke-width=\"2px\" d=\"M152,182.0 152,152.0 404.0,152.0 404.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-72c9ea1ee15747e0ac9714f1803f2384-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M152,184.0 L148,176.0 156,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-72c9ea1ee15747e0ac9714f1803f2384-0-2\" stroke-width=\"2px\" d=\"M152,182.0 152,167.0 221.0,167.0 221.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-72c9ea1ee15747e0ac9714f1803f2384-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M221.0,184.0 L225.0,176.0 217.0,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-72c9ea1ee15747e0ac9714f1803f2384-0-3\" stroke-width=\"2px\" d=\"M332,182.0 332,167.0 401.0,167.0 401.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-72c9ea1ee15747e0ac9714f1803f2384-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M332,184.0 L328,176.0 336,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-72c9ea1ee15747e0ac9714f1803f2384-0-4\" stroke-width=\"2px\" d=\"M422,182.0 422,167.0 491.0,167.0 491.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-72c9ea1ee15747e0ac9714f1803f2384-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M422,184.0 L418,176.0 426,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-72c9ea1ee15747e0ac9714f1803f2384-0-5\" stroke-width=\"2px\" d=\"M512,182.0 512,167.0 581.0,167.0 581.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-72c9ea1ee15747e0ac9714f1803f2384-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M512,184.0 L508,176.0 516,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-72c9ea1ee15747e0ac9714f1803f2384-0-6\" stroke-width=\"2px\" d=\"M692,182.0 692,152.0 854.0,152.0 854.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-72c9ea1ee15747e0ac9714f1803f2384-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M692,184.0 L688,176.0 696,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-72c9ea1ee15747e0ac9714f1803f2384-0-7\" stroke-width=\"2px\" d=\"M782,182.0 782,167.0 851.0,167.0 851.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-72c9ea1ee15747e0ac9714f1803f2384-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M782,184.0 L778,176.0 786,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-72c9ea1ee15747e0ac9714f1803f2384-0-8\" stroke-width=\"2px\" d=\"M602,182.0 602,137.0 857.0,137.0 857.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-72c9ea1ee15747e0ac9714f1803f2384-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M857.0,184.0 L861.0,176.0 853.0,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-72c9ea1ee15747e0ac9714f1803f2384-0-9\" stroke-width=\"2px\" d=\"M962,182.0 962,167.0 1031.0,167.0 1031.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-72c9ea1ee15747e0ac9714f1803f2384-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M962,184.0 L958,176.0 966,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-72c9ea1ee15747e0ac9714f1803f2384-0-10\" stroke-width=\"2px\" d=\"M872,182.0 872,152.0 1034.0,152.0 1034.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-72c9ea1ee15747e0ac9714f1803f2384-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1034.0,184.0 L1038.0,176.0 1030.0,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-72c9ea1ee15747e0ac9714f1803f2384-0-11\" stroke-width=\"2px\" d=\"M1142,182.0 1142,152.0 1394.0,152.0 1394.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-72c9ea1ee15747e0ac9714f1803f2384-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1142,184.0 L1138,176.0 1146,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-72c9ea1ee15747e0ac9714f1803f2384-0-12\" stroke-width=\"2px\" d=\"M1232,182.0 1232,167.0 1301.0,167.0 1301.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-72c9ea1ee15747e0ac9714f1803f2384-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1232,184.0 L1228,176.0 1236,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-72c9ea1ee15747e0ac9714f1803f2384-0-13\" stroke-width=\"2px\" d=\"M1322,182.0 1322,167.0 1391.0,167.0 1391.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-72c9ea1ee15747e0ac9714f1803f2384-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1322,184.0 L1318,176.0 1326,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-72c9ea1ee15747e0ac9714f1803f2384-0-14\" stroke-width=\"2px\" d=\"M602,182.0 602,122.0 1400.0,122.0 1400.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-72c9ea1ee15747e0ac9714f1803f2384-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1400.0,184.0 L1404.0,176.0 1396.0,176.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Constituency Parsing Tree (Text Representation):\n",
            "(S (NP (NP (FW nisei) (NN femal)) (VP (VP (VBN born) (MD may) (RB selleck)) (NNP washington))) (VP (VBD spent) (NP (JJ much) (NN childhood)) (NP (NP (UCP (NN beaverton) (FW oregon)) (NN father)) (JJ own) (NN farm) (FW influenc) (FW earli))))\n",
            "                                                       S                                                                    \n",
            "                  _____________________________________|_________________________________                                    \n",
            "                 |                                                                       VP                                 \n",
            "                 |                                 ______________________________________|____________                       \n",
            "                 NP                               |         |                                         NP                    \n",
            "        _________|________                        |         |                             ____________|__________________    \n",
            "       |                  VP                      |         |                            NP           |   |      |       |  \n",
            "       |               ___|______________         |         |                        ____|______      |   |      |       |   \n",
            "       NP             VP                 |        |         NP                     UCP          |     |   |      |       |  \n",
            "   ____|____      ____|_________         |        |     ____|______           ______|____       |     |   |      |       |   \n",
            "  FW        NN  VBN   MD        RB      NNP      VBD   JJ          NN        NN          FW     NN    JJ  NN     FW      FW \n",
            "  |         |    |    |         |        |        |    |           |         |           |      |     |   |      |       |   \n",
            "nisei     femal born may     selleck washington spent much     childhood beaverton     oregon father own farm influenc earli\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OyrRMCXwMwN",
        "outputId": "5e03a950-3ceb-4f20-ba94-80c5500e91b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entity Recognition Results are: \n",
            "PERSON: 777 Entities\n",
            "GPE: 1523 Entities\n",
            "DATE: 237 Entities\n",
            "ORG: 238 Entities\n",
            "PRODUCT: 1 Entities\n"
          ]
        }
      ],
      "source": [
        "########################### PART III ##################################\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load the CSV file\n",
        "df_NER = pd.read_csv('/content/Narrators_Information_Cleaned.csv')\n",
        "\n",
        "# Initialize counters and entity storage\n",
        "entity_counter = Counter()\n",
        "entity_examples = {\"PERSON\": [], \"ORG\": [], \"GPE\": [], \"PRODUCT\": [], \"DATE\": []}\n",
        "\n",
        "# Extract entities from the \"CleanedDetails\" column\n",
        "for i in df[\"CleanedDetails\"]:\n",
        "    doc = nlp(i)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in entity_examples:\n",
        "            entity_counter[ent.label_] += 1\n",
        "            #entity_examples[ent.label_].append(ent.text)\n",
        "\n",
        "\n",
        "\n",
        "# Print results\n",
        "print('Named Entity Recognition Results are: ')\n",
        "for entity, count in entity_counter.items():\n",
        "    print(f\"{entity}: {count} Entities\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcVqy1yj3wja"
      },
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEdcyHX8VaDB"
      },
      "source": [
        "#Question 4 (20 points)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ung5_YW3C6y"
      },
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub’s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTOfUpatronW"
      },
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hdOwLZyJVmB"
      },
      "outputs": [],
      "source": [
        "#################### Part-1 ###################\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "from urllib.error import HTTPError\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "NoOfPages = 55\n",
        "Dlist = []\n",
        "with open('github_marketplace_products1.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "  writer = csv.DictWriter(file, fieldnames=['Product Name', 'Description', 'URL', 'Page Number'])\n",
        "  writer.writeheader()\n",
        "  for j in range(1,NoOfPages+1):\n",
        "    Git_URL = f\"https://github.com/marketplace?page={j}&type=actions\"\n",
        "\n",
        "    GitLink = Request(Git_URL, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    GitURL = urlopen(GitLink)\n",
        "\n",
        "    GitResponse = GitURL.read()\n",
        "      #data1_soup = BeautifulSoup(data1)\n",
        "\n",
        "    GitSoup = BeautifulSoup(GitResponse,\"html.parser\")\n",
        "    #print(soup.prettify())\n",
        "    #Action_Div = soup.find_all(class_='flex-1')\n",
        "    #print(type(Action_Div))\n",
        "\n",
        "    Action_Div = GitSoup.find_all(class_=\"position-relative border rounded-2 d-flex marketplace-common-module__marketplace-item--MohVH gap-3 p-3\")\n",
        "    #print(Action_Div)\n",
        "    for i in Action_Div:\n",
        "      name_tag = i.find(class_=\"marketplace-common-module__marketplace-item-link--jrIHf line-clamp-1\")\n",
        "      productName = name_tag.get_text().strip()\n",
        "      desc_tag = i.find('p')\n",
        "      productDescription = desc_tag.get_text().strip()\n",
        "      URL_tag = name_tag['href']\n",
        "      ProductURL = 'https://github.com/'+str(URL_tag)\n",
        "      writer.writerow({\n",
        "                'Product Name': productName,\n",
        "                'Description': productDescription,\n",
        "                'URL': ProductURL,\n",
        "                'Page Number': j\n",
        "            })\n",
        "      time.sleep(0.2)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJunoelAuTpt",
        "outputId": "a561d16e-160f-44df-f3d4-ce0355240434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No missing data.\n",
            "\n",
            "Cleaned Data Sample:\n",
            "                   Product Name  \\\n",
            "0                TruffleHog OSS   \n",
            "1                 Metrics embed   \n",
            "2  yq - portable yaml processor   \n",
            "3                  Super-Linter   \n",
            "4        Gosec Security Checker   \n",
            "\n",
            "                                         Description  \\\n",
            "0                      scan github action trufflehog   \n",
            "1  infographics generator plugins option display ...   \n",
            "2      create read update delete merge validate yaml   \n",
            "3  superlinter readytorun collection linters code...   \n",
            "4                         run gosec security checker   \n",
            "\n",
            "                                                 URL  Page Number  Valid_URL  \n",
            "0  https://github.com//marketplace/actions/truffl...            1       True  \n",
            "1  https://github.com//marketplace/actions/metric...            1       True  \n",
            "2  https://github.com//marketplace/actions/yq-por...            1       True  \n",
            "3  https://github.com//marketplace/actions/super-...            1       True  \n",
            "4  https://github.com//marketplace/actions/gosec-...            1       True  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "###################### part2 ######################\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "\n",
        "df_git = pd.read_csv('github_marketplace_products1.csv')\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "###################  PREPROCESSING ##########################\n",
        "# Initialize Lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# function to do all preprocessing\n",
        "def preprocess_text(text):\n",
        "\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)  # Convert non-string types to string\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and digits (keeping only letters and spaces)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and apply lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Join tokens back into a single string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing on 'Description' column\n",
        "df_git['Description'] = df_git['Description'].apply(preprocess_text)\n",
        "\n",
        "###################  DATA QUALITY CHECKS ##########################\n",
        "\n",
        "\n",
        "# 1. Check for missing values\n",
        "missing_data = df_git.isnull().sum()\n",
        "\n",
        "\n",
        "# 2. Remove duplicates based on 'Product Name' and 'URL' columns\n",
        "df_git = df_git.drop_duplicates(subset=['Product Name', 'URL'])\n",
        "\n",
        "# 3. Ensure that 'Product Name' and 'URL' are not empty\n",
        "df_git = df_git[df_git['Product Name'].notna() & df_git['URL'].notna()]\n",
        "\n",
        "# 4. Check for invalid URLs (basic check)\n",
        "url_pattern = re.compile(r'^(https?://)?(www\\.)?github\\.com/.+')\n",
        "df_git['Valid_URL'] = df_git['URL'].apply(lambda x: bool(url_pattern.match(x)))\n",
        "\n",
        "\n",
        "\n",
        "# Filter out rows with invalid URLs\n",
        "df_git = df_git[df_git['Valid_URL']]\n",
        "\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "df_git.to_csv('cleaned_github_marketplace_products.csv', index=False)\n",
        "\n",
        "\n",
        "if missing_data.any():\n",
        "    print(\"There is missing data.\")\n",
        "else:\n",
        "    print(\"No missing data.\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nCleaned Data Sample:\")\n",
        "print(df_git.head())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WeD70ty3Gui"
      },
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYRO5Cn8bYwZ"
      },
      "outputs": [],
      "source": [
        "############################# PART I #################################################\n",
        "import tweepy\n",
        "\n",
        "# Set your keys and tokens here\n",
        "api_key = 'HLWL67RpwHb5ZF450wNnjHGuE'\n",
        "api_key_secret = 'gzvolj1ISwrCXCbCwDYEZIozsGQHlC3tbQhh5vF4hInSqE95jq'\n",
        "access_token = '1892332327445102592-LKi5zuQKTyxUjHGg9NBtnPBFZRUZmL'\n",
        "access_token_secret = 'XZaaPy3WpYTgMLPxWLjoa3njWp6mU3dlI6t62wlzBaF5k'\n",
        "\n",
        "# Authenticate with Twitter\n",
        "auth = tweepy.OAuth1UserHandler(\n",
        "    consumer_key=api_key,\n",
        "    consumer_secret=api_key_secret,\n",
        "    access_token=access_token,\n",
        "    access_token_secret=access_token_secret\n",
        ")\n",
        "api = tweepy.API(auth)\n",
        "max_tweets = 1000\n",
        "hashtag = '#generativeAI -is:retweet'\n",
        "\n",
        "client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAADagzQEAAAAAg1n8bETsGPF1BGuI7rcaKzQgEFg%3D1wm1GnUryrzjA6gg6ulcEMHeW3sIS2EoLop2TgrJGhduzffO1d')\n",
        "tweets = client.search_recent_tweets(query=hashtag, tweet_fields=[\"created_at\", \"text\", \"author_id\"], max_results=100)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCrPXCzYPhbR",
        "outputId": "16a0279f-a80f-4b3f-fdd9-33b81f5301e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              tweet_id              user_id                tweet_time  \\\n",
            "0  1892333447869317230  1637311944008044544 2025-02-19 22:00:26+00:00   \n",
            "1  1892333446673928615             50065957 2025-02-19 22:00:26+00:00   \n",
            "2  1892333426964881545  1829231426673266688 2025-02-19 22:00:21+00:00   \n",
            "3  1892333322162098357  1219122060531658754 2025-02-19 21:59:56+00:00   \n",
            "4  1892333208886259740  1724183495122079744 2025-02-19 21:59:29+00:00   \n",
            "\n",
            "                                           tweetText  \n",
            "0  Image FX 自由研究その199\\n[フェンシング女子🥰]\\n\\nCompetitive...  \n",
            "1  How \"responsible\" are your org's marketing pra...  \n",
            "2  Connected Intelligence, Boomi Locks Down APIs ...  \n",
            "3  Fascinated by the fusion of art and technology...  \n",
            "4  Microsoft touts Muse, a new generative AI mode...  \n"
          ]
        }
      ],
      "source": [
        "#print(tweets)\n",
        "# Create a dictionary to store tweet data\n",
        "tweetDict= {'tweet_id': [],'user_id': [],'tweet_time': [],'tweetText': []}\n",
        "\n",
        "if tweets.data:\n",
        "    for i in tweets.data:\n",
        "        tweetDict['tweet_id'].append(i.id)\n",
        "        tweetDict['user_id'].append(i.author_id)\n",
        "        tweetDict['tweet_time'].append(i.created_at)\n",
        "        tweetDict['tweetText'].append(i.text)\n",
        "\n",
        "# dataframe creation\n",
        "dfTweets = pd.DataFrame(tweetDict)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(dfTweets.head())\n",
        "\n",
        "# Save DataFrame to a CSV file\n",
        "dfTweets.to_csv(\"Generative_AI_Tweets.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfbQxa3jSygc",
        "outputId": "d9a80faf-a443-4f80-f326-bbe3ee4b867b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No missing data.\n",
            "\n",
            "Cleaned Data Sample:\n",
            "              tweet_id              user_id                 tweet_time  \\\n",
            "0  1892333447869317230  1637311944008044544  2025-02-19 22:00:26+00:00   \n",
            "1  1892333446673928615             50065957  2025-02-19 22:00:26+00:00   \n",
            "2  1892333426964881545  1829231426673266688  2025-02-19 22:00:21+00:00   \n",
            "3  1892333322162098357  1219122060531658754  2025-02-19 21:59:56+00:00   \n",
            "4  1892333208886259740  1724183495122079744  2025-02-19 21:59:29+00:00   \n",
            "\n",
            "                                           tweetText  \n",
            "0  image fx competitive fencing tournament ai aia...  \n",
            "1  responsible orgs marketing practice true compe...  \n",
            "2  connected intelligence boomi lock apis generat...  \n",
            "3  fascinated fusion art technology explore new g...  \n",
            "4  microsoft tout muse new generative ai model vi...  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "######################### Part II ##########################\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "\n",
        "df_Tweet = pd.read_csv('/content/Generative_AI_Tweets.csv')\n",
        "#print(df_Tweet.describe)\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "###################  PREPROCESSING ##########################\n",
        "# Initialize Lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# function to do all preprocessing\n",
        "def preprocess_text_1(text):\n",
        "\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)  # Convert non-string types to string\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and digits (keeping only letters and spaces)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and apply lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Join tokens back into a single string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing on 'Description' column\n",
        "df_Tweet['tweetText'] = df_Tweet['tweetText'].apply(preprocess_text_1)\n",
        "\n",
        "###################  DATA QUALITY CHECKS ##########################\n",
        "\n",
        "\n",
        "# 1. Check for missing values\n",
        "missing_data_1 = df_Tweet.isnull().sum()\n",
        "\n",
        "# 2. Remove duplicates\n",
        "df_Tweet = df_Tweet.drop_duplicates(subset=['tweetText', 'user_id'])\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "df_Tweet.to_csv('cleaned_Generative_AI_Tweets.csv', index=False)\n",
        "\n",
        "\n",
        "if missing_data_1.any():\n",
        "    print(\"There is missing data.\")\n",
        "    df_Tweet['tweetText'].fillna('Unknown', inplace=True)\n",
        "    df_Tweet['user_id'].fillna('Unknown', inplace=True)\n",
        "    df_Tweet['author_id'].fillna('No information available',inplace=True)\n",
        "\n",
        "else:\n",
        "    print(\"No missing data.\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nCleaned Data Sample:\")\n",
        "print(df_Tweet.head())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment was good but it was hard. The good thing is this assignment helps to learn how to webscrap any website."
      ],
      "metadata": {
        "id": "ywc2n0q1u31U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbTa-jDS-KFI"
      },
      "source": [
        "# Write your response below\n",
        "Fill out survey and provide your valuable feedback.\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSd_ObuA3iNoL7Az_C-2NOfHodfKCfDzHZtGRfIker6WyZqTtA/viewform?usp=dialog"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}